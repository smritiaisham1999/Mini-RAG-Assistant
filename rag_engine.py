import os
import logging
import numpy as np
from langchain_community.document_loaders import PyPDFLoader, TextLoader, Docx2txtLoader
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI
from langchain_community.vectorstores import FAISS
from langchain_classic.text_splitter import RecursiveCharacterTextSplitter
from langchain_classic.prompts import PromptTemplate

from langchain_classic.schema import Document

# Setup Logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Folder to save the database
DB_PATH = "faiss_db_store"

class RAGManager:
    def __init__(self):
        self.vector_store = None

    def _get_embeddings(self, provider, api_key):
        if provider == "openai":
            return OpenAIEmbeddings(model="text-embedding-3-small", openai_api_key=api_key)
        elif provider == "gemini":
            return GoogleGenerativeAIEmbeddings(model="gemini-embedding-001", google_api_key=api_key)
        return None
    
    def _get_llm(self, provider, api_key, temperature=0.3):
        # Temperature 0.3 allows for better synthesis of definitions
        if provider == "openai":
            return ChatOpenAI(model="gpt-4o-mini", temperature=temperature, openai_api_key=api_key)
        elif provider == "gemini":
            return ChatGoogleGenerativeAI(model="gemini-2.5-flash", temperature=temperature, google_api_key=api_key)
        return None

    def _calculate_confidence(self, distance):
        """
        DEMO-READY CONFIDENCE SCORING
        """
        try:
            dist = float(distance)
            if dist < 0: dist = 0.0
            # Formula: 1 / (1 + (dist * 0.3)) -> Balanced scoring
            score = 1.0 / (1.0 + (dist * 0.3))
            return float(round(score * 100, 2))
        except:
            return 0.0

    def _format_history(self, history_list):
        history_text = ""
        for msg in history_list:
            role = "User" if msg['role'] == 'user' else "Assistant"
            content = str(msg['content'])
            history_text += f"{role}: {content}\n"
        return history_text if history_text else "No previous chat history."

    def load_existing_db(self, provider, api_key):
        if os.path.exists(DB_PATH):
            embeddings = self._get_embeddings(provider, api_key)
            try:
                self.vector_store = FAISS.load_local(DB_PATH, embeddings, allow_dangerous_deserialization=True)
                print("✅ Database loaded successfully.")
            except Exception as e:
                print(f"⚠️ Database load error: {e}")
                self.vector_store = None

    def process_files(self, file_paths, username, privacy, provider, api_key):
        documents = []
        for file_path in file_paths:
            file_name = os.path.basename(file_path)
            try:
                if file_path.endswith(".pdf"):
                    loader = PyPDFLoader(file_path)
                elif file_path.endswith(".docx"):
                    loader = Docx2txtLoader(file_path)
                else:
                    loader = TextLoader(file_path)
                
                docs = loader.load()
                for doc in docs:
                    doc.metadata["source"] = file_name
                    doc.metadata["owner"] = username
                    doc.metadata["privacy"] = privacy
                documents.extend(docs)
            except Exception as e:
                print(f"❌ Error processing {file_name}: {e}")
                continue

        if not documents:
            return 0

        # Chunk Size: 1000 chars is optimal
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
        splits = text_splitter.split_documents(documents)

        embeddings = self._get_embeddings(provider, api_key)

        if self.vector_store is None:
            if os.path.exists(DB_PATH):
                try:
                    self.vector_store = FAISS.load_local(DB_PATH, embeddings, allow_dangerous_deserialization=True)
                    self.vector_store.add_documents(splits)
                except:
                    self.vector_store = FAISS.from_documents(splits, embeddings)
            else:
                self.vector_store = FAISS.from_documents(splits, embeddings)
        else:
            self.vector_store.add_documents(splits)
        
        self.vector_store.save_local(DB_PATH)
        return len(splits)

    # --- THIS WAS MISSING BEFORE ---
    def _generate_query_variations(self, original_query, llm):
        """
        Generates synonyms/variations to improve search recall.
        """
        prompt = PromptTemplate(
            input_variables=["question"],
            template="""Generate 2 synonyms or related questions for: "{question}".
            Example: "Cost" -> "Price", "Budget".
            Keep it simple. Return only the questions separated by newlines."""
        )
        try:
            response = llm.invoke(prompt.format(question=original_query))
            variations = response.content.split('\n')
            cleaned = [v.strip() for v in variations if v.strip()]
            return cleaned[:2] 
        except:
            return [original_query]

    def get_answer(self, query, history, username, provider, api_key):
        embeddings = self._get_embeddings(provider, api_key)
        llm = self._get_llm(provider, api_key, temperature=0.3)
        
        if self.vector_store is None:
            self.load_existing_db(provider, api_key)

        if not self.vector_store:
            return {"answer": "⚠️ Database is empty.", "sources": [], "confidence": 0.0}

        # --- STEP 1: SEARCH ---
        queries_to_search = [query]
        if len(query.split()) < 10:
            variations = self._generate_query_variations(query, llm)
            queries_to_search.extend(variations)

        unique_docs = {}
        for q in queries_to_search:
            docs_and_scores = self.vector_store.similarity_search_with_score(q, k=4)
            for doc, distance in docs_and_scores:
                doc_owner = doc.metadata.get("owner", "unknown")
                doc_privacy = doc.metadata.get("privacy", "private")
                has_access = (doc_owner == username) or (doc_privacy == "public")
                
                if has_access:
                    content_hash = hash(doc.page_content)
                    if content_hash not in unique_docs:
                        unique_docs[content_hash] = (doc, distance)
                    else:
                        if distance < unique_docs[content_hash][1]:
                            unique_docs[content_hash] = (doc, distance)

        results = list(unique_docs.values())
        results.sort(key=lambda x: x[1])
        top_results = results[:3]

        if not top_results:
             return {"answer": "I couldn't find relevant info.", "sources": [], "confidence": 0.0}

        # --- STEP 2: SCORES ---
        best_distance = float(top_results[0][1])
        try:
            score_val = 1.0 / (1.0 + (best_distance * 0.3))
            confidence = float(round(score_val * 100, 2))
        except:
            confidence = 0.0
        
        context_text = ""
        sources = []
        for doc, dist in top_results:
            try:
                s_score = 1.0 / (1.0 + (float(dist) * 0.3))
                s_score = float(round(s_score * 100, 2))
            except: 
                s_score = 0.0
                
            source_label = doc.metadata.get('source', 'Unknown')
            context_text += f"\n---\n[Source: {source_label}]\nContent: {doc.page_content}\n"
            sources.append({"source": source_label, "content": doc.page_content, "score": s_score})

        avg_precision = sum([s['score'] for s in sources]) / len(sources) if sources else 0.0

        # --- STEP 3: PROMPT ---
        formatted_history = self._format_history(history)
        
        prompt_template = """
        You are a smart Corporate RAG Assistant. 
        Your goal is to answer the user's question using the Context provided.

        ### INSTRUCTIONS:
        1. **Check Context:** If the answer is in the text, explain it clearly.
        2. **Definitions:** If asked "What is X?" and the text describes X, that is the answer.
        3. **Negative Answers:** If the context is NOT related to the question (e.g., Question about "Elon Musk" but Context is about "RAG"), strictly say: "I cannot find this information in the provided documents."
        4. **Citation:** Append [Source Name] at the end.

        Chat History:
        {history}

        Context:
        {context}

        Question: {question}

        Answer:
        """
        
        prompt = PromptTemplate(template=prompt_template, input_variables=["history", "context", "question"])
        final_prompt = prompt.format(history=formatted_history, context=context_text, question=query)
        
        try:
            response = llm.invoke(final_prompt)
            answer_text = response.content
        except Exception as e:
            answer_text = f"Error from LLM: {str(e)}"

        # --- STEP 4: SMART OVERRIDE ---
        # If the LLM says it can't find info, force confidence to 0
        keywords = ["cannot find", "no information", "not mentioned", "does not contain"]
        if any(k in answer_text.lower() for k in keywords):
            confidence = 0.0
            avg_precision = 0.0
            sources = [] 

        return {
            "answer": answer_text,
            "sources": sources,
            "confidence": confidence,
            "retrieval_quality": float(round(avg_precision, 2))
        }

